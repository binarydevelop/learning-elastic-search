## Managing Documents
- As elasticsearch exposes REST Api's we can use those verbs on our resources to manage indexes and documents. 
---
### Creating and Deleting Indices 
- Dleting an Index: 
```JS

DELETE /index_name
```
- Creating an Index.<br/>
 To define the index settings we need to add a request body. The index settings should be specified within a nested object named "settings" 
```JS
PUT /index_name #creates index with default settings.

PUT /index_name 
{
    "settings": {
        "number_of_shards": 2,
        "number_of_replicas": 2
    }
}
```

---
### Indexing Documents 
- We can index a document by sending a POST request to an endpoint consisting of the index name, a slash, and "_doc".
We then need to define the document within the request body as a JSON object.
```JS
POST /index_name/_doc
{
    "name": "test"
     ...
}
```
- If we do not provide Id an auto-generated id will be assigned.

```JS
PUT /index_name/_doc/10
{
    "name": "test"
     ...
}
```

### Retrieving documents
```
GET /index_name/_doc/10
```
- This will retrieve a document with id 10.

### Updating Documents
```
POST /index_name/_doc/10 
{
    "doc": {
        "name" : "changed-name",
    }
}
```

This will result in something like this: 
```
{
  "_index" : "products",
  "_type" : "_doc",
  "_id" : "10",
  "_version" : 2,
  "result" : "updated",
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "failed" : 0
  },
  "_seq_no" : 2,
  "_primary_term" : 1
}
```
- The key "result" will say noop if there had been no update. </br>
Apart from updating existing fields, we can also add new fields to existing documents.
We do that in exactly the same way and just add the new field.  
---

- Elasticsearch documents are immutable what this means is we cannot change the documents the POST request simply replaces the old one with the same id.

---

### Scripting

- Elasticsearch supports scripting which enables you to write custom logic while accessing a document's values.
```
POST /products/_update/10 
{
  "script": {
    "source": "ctx._source.in_stock--"
  }
}
```
- "ctx" is a variable, and is short for "context." We can access the source document
through its "_source" property, which gives us an object containing the document's fields.

- We can also define parameters within the request. We define parameters within an option named "params," which is an object consisting
of key-value pairs, where each pair corresponds to a parameter name and value.

- If we update a field using scripts the "result" key will always be updated even if no fields change. We can do this by setting ```ctx.op = "noop"```

```
POST /products/_update/10 
{
  "script": {
    "source": """
    if(ctx._source.in_stock == 76) {
      ctx.op ='noop';
    } else {
      ctx._source.in_stock *= params.quantity
    }
    """,
    "params": {
      "quantity": 4
    }
  }
}
```

---

### Upserts
- Upserts is another way of updating the documents. Upserting means to conditionally update or insert a document based on whether or not the document already exists. So if the document already exists, a script is run, and if it doesn't, a new document is indexed. 

```JS
// Upsert
POST /products/_update/101
{
  "script": {
    "source": """
    if(ctx._source.in_stock == 76) {
      ctx.op ='noop';
    } else {
      ctx._source.in_stock *= params.quantity
    }
    """,
    "params": {
      "quantity": 4
    }
  },
   "upsert": {
      "name":"upserted-document",
      "description": "upserted-description",
      "tags": ["upserted"]
    }
}
```

- The script is run if the document already exists otherwise the contents of the "upsert" option is added as a new document.
---
- Updating will just replace the documents.
- Deleting is similar.
```
#Replacing 
#POST /products/_update/101
{
    fields..
}

# Deleting 
DELETE /products/_doc/101
```
---

### Routing
- We know the VERBS to manage documents. But <span style="color:yellow"> How does it work under the hood? </span>
Particularly, how did Elasticsearch know on which shard to store the documents?</br>
 And <span style="color:yellow">How did it know on which shard to find an existing document, be it to retrieve, update, or delete it?</span>

 - The answer to both those questions is routing. In its basic form, routing refers to the process of resolving a document's shard, both in the context of retrieving the document, but also to figure out on which shard it should be stored in the first place.</br>
 When we index a new document, Elasticsearch uses a simple formula to calculate on which shard the document should be stored. </br>
 ``` shard_num = hash(_routing) % num_primary_shards ```

Here _routing is document's id. 
- What happened when we retrieved a document by its ID, was that the same formula was used based on the ID that we specified. Elasticsearch then knew that if the document existed, it
had to be on the shard that the routing formula yielded. The same applies for updating and
deleting specific documents. Searching for documents based on some criteria other than
their IDs, works differently.

---
<span style="color:yellow">How Elasticsearch reads Data? </span>
- The very first thing that happens, is that a given node receives the read request. This node is responsible for coordinating the request, and is therefore referred to
as the coordinating node.
So what does this coordination involve?<br/>
The first step is to figure out where the document we are looking for is stored. That's done with routing.
- Routing resolves to the shard that stores
a given document but more specifically it resolves to a primary shard (or a replication group).
A shard is chosen from the replication group.
Elasticsearch uses a technique called Adaptive Replica Selection (ARS) for this purpose.
What ARS essentially does, is to select the shard copy that is deemed to be the best.
This is evaluated by a formula that takes a number of factors into account.<br/>

<span style="color:yellow">
How does it write data? </span>

- First of all, the request goes through the same routing process. The request is resolved to the replication group that stores or should store  the document.<br/>
 Instead of routing the request to any of the shards within the replication group, write requests are always routed to the primary shard.
- The primary shard is first of all responsible for validating the request.
This involves validating the structure of the request, as well as validating field values.
- The primary shard then performs the write operation locally, before forwarding it to
the replica shards to keep those up to date as well.
To improve performance, the primary shard forwards the operation to its replica shards
in parallel.
--- 
### Handling Failures
To counter failures in replication elasticsearch uses something called sequence number and primary terms. 

- Primary terms are a way for Elasticsearch to distinguish between old and new primary shards when the primary shard of a replication group has changed. <br/>
The primary term for a replication group is essentially just a counter for how many times the primary shard has changed.

- What happens as part of write operations, is that the current primary term is appended to the operations that are sent to the replica shards. This enables the replica shards to tell whether or not the primary shard has changed since.
- Apart from associating each operation with a primary term, a sequence number is also given to operations.
This sequence number is essentially just a counter that is incremented for each operation,
at least until the primary shard changes. Sequence numbers enable Elasticsearch to know in which order operations happened on a given primary shard. 

- Elasticsearch maintains global and local checkpoints. Both of these checkpoints are essentially sequence numbers. <br/>
A global checkpoint exists for each replication group, while a local checkpoint is kept for each replica shard.

- The global checkpoint is the sequence number that all of the active shards within a replication group have been aligned at least up to. This means that any operations containing a sequence number lower than the global checkpoint,
have already been performed on all shards within the replication group.<br/>
If a primary shard fails and rejoins the cluster at a later point, Elasticsearch only needs
to compare the operations that are above the global checkpoint that it last knew about.
Likewise, if a replica shard fails, only the operations that have a sequence number higher
than its local checkpoint need to be applied when it comes back.
--- 
### Versioning
- Elasticsearch actually versions the documents that we index.
What Elasticsearch does, is to store an "_version" metadata field together with the documents that we index.

- The field's value starts at one, and is incremented by one every time a document is updated or deleted. In the case of deleting a document, Elasticsearch will retain the version number for 60 seconds by default. If we index a new document with the same ID within 60 seconds, the version will be incremented; otherwise it will be reset to one. If you retrieve a document by its ID, you will see this metadata field within the response. For search requests - which we will get to later in the course - the field is not included within the results automatically, but Elasticsearch can easily be instructed to do so.

- This is the default type of versioning that is used, and is referred to as "internal"versioning. There is another type of versioning that we can use, which is called "external" versioning. This versioning type is meant for situations where you maintain a document's version outside of Elasticsearch, such as within a database. An example could be that you use a relational database as the primary data store, and index data into Elasticsearch to make it searchable.
To use external versioning, you specify both the version that Elasticsearch should store, as well as the version type. The version that you specify, is constrained to being a natural number. 
Example : External Versioning. 

```
PUT /products/_docs/123?version=521&version_type=external
{
  fields..
  ...
}
```
---
Optimistic Concurrency Control

- This is essentially a way to prevent that an old version of a document overwrites a more recent one, i.e. if write operations arrive out of sequence.

-  A simple example:  when a web application updates a field.
More specifically, an ecommerce application.
Suppose that a website visitor has added an item to the cart, and completed the checkout flow.
Once that happens, the application retrieves the product from Elasticsearch.
At this particular point in time, another visitor completes the checkout flow for the
same product, and a different thread on the web server also retrieves the product.
At this point, both threads have retrieved the same product.
The first thread subtracts the product's "in_stock" field by one and updates the
product through the Elasticsearch API.
The second thread does the same thing, and this is where we run into trouble. The second thread thinks it has the latest value for the "in_stock" field, but it has been updated from six to five since the product was retrieved. One is subtracted from the incorrect value, and the product is updated again, but with the same field value. There are no errors indicating that anything went wrong, and the application believes everything is okay.
The "in_stock" field, however, now has a value of five, where the value should have been four.

<span style="color:yellow; font-size:14px">
So how do we prevent this from happening?
</span>

- In old ways this is where versions came into play. 
We would send the version of the document as the query parameter and thus match it with the version number in the elastic document as well. 

![Old Way](images/old-way.png)

- The new way involves using primary term and sequence term .
When we retrieve the product, the current primary term and sequence number are included within the results. <br/>
What we can do, is to take the values and add them to the POST request that we send to update the document. To do that, we use the "if_seq_no" and "if_primary_term" parameters.
Elasticsearch will then use these two values to ensure that we won't overwrite a document inadvertently if it has changed since we retrieved it.
---

### Update By query 

```JS
#update by query 
POST /products/_update_by_query
{
  "script": {
    "source": "ctx._source.in_stock--",
    "lang": "painless"
  },
  "query": {
    "match_all": {}
  }
}
```
- ```query``` is provided to match documents under a condition and update them accordingly.
---
### Delete By Query 
```JS
#Delete by query 
POST /products/_delete_by_query
{
  "query": {
    "match_all": {}
  }
}
```
- Notice that POST verb is used instead of DELETE. This will delete all the documents inside products. 
---

### Batch Processing 

- Batch Processing is done using the Bulk Api. The Bulk Api expects data formatted using NDJSON specification. 
```
action_and_metadata \n 
optionl_source \n
action_and_metadata \n 
optionl_source \n
```
- Let's begin by indexing a new document. First, we define the HTTP verb and the endpoint.
Next, we need to define the action that we want Elasticsearch to perform. We describe that within a JSON object.
- We have four actions to choose from; "index," "create," "update," and "delete."
- Let's begin with the "index" action. The action should
simply be defined as a key within this object, and the value for the key should be a JSON object. With index action Elasticsearch would know that we want to index a document.The value for this would be typically the metadata for the action. This will typically be the index name the document should be added to, and the ID of the document, which are specified with the "_index" and "_id" keys, respectively. 

- Specifying the document's ID is optional; if you leave it out, Elasticsearch will automatically generate one for you.
we define the fields and values that the document should contain on a new line, which will also be a JSON object.
Within this object, we simply add key-value pairs, where the keys are the field names, and the values are the field values. 

- The "create" action will fail if the document already exists, which is not the case for the "index" action. If you use the "index" action, the document will be added if it doesn't already exist; otherwise it will be replaced.

- We can either define the index in the request itself or we can define it as an action metadata.
```
POST /_bulk 
{ "index": {"_index": "products", "_id": 201 } }
{ "name":"espresso", "price":199, "in_Stock":56}
{ "create": {"_index": "products", "_id": 202 } }
{ "name":"fabico", "price":1299, "in_Stock":546}
```
- Updating & Deleting 

```
POST /_bulk 
{ "update": {"_index": "products", "_id": 201 } }
{ doc: {"name":"espresso2"} }
{ "delete": {"_index": "products", "_id": 200 } }
```
- If all actions are for the same index, however, we can specify the index name within the request path instead.
```
POST /products/_bulk 
{ "update": { "_id": 201 } }
{ doc: {"name":"espresso2"} }
{ "delete": { "_id": 200 } }
```

---
### Things to be aware of while using _bulk
- First, the "Content-Type" header of the HTTP request should be set to the value 
``` Content-Type : application/x-ndjson ```

- Each of the lines must end with \n or \r\n or simply enter key. Even on the last line. 

- If a single action fails, this will not affect the other actions, as they will proceed as normal. That's why Elasticsearch returns detailed information about every action within the "items" key, 
---
### Importing Data with Curl 
- Curl is an HTTP client, which is the most popular command-line tool for sending HTTP requests.
```
curl -H "Content-Type: application/x-ndjson" -X POST "http://localhost:9200/products/_bulk" --data-binary "@products-bulk.json"
```
---