### Analysis 

- Referred to as text analysis. That’s because the concept is really only applicable to text values. When we index a text value, it goes through an analysis process. </br>
The purpose of it is to store the values in a data structure that is efficient for searching. The text is also processed before being stored. 

- When a text value is indexed, a so-called analyzer is used to process the text. In other words, the value is analyzed.
An analyzer consists of three building blocks; character filters, a tokenizer, and token filters.

- The result of analyzing text values is then stored in a searchable data structure.
Let’s go over the parts of an analyzer one by one, beginning with character filters.
A character filter receives the original text and may transform it by adding, removing,
or changing characters.
An analyzer may have zero or more character filters, and they are applied in the order
in which they are specified.
An example could be to remove HTML elements and convert HTML entities by using a character
filter named “html_strip.”

- An analyzer must contain exactly one tokenizer, which is responsible for tokenizing the text.
By “tokenizing,” I am referring to the process of splitting the text into tokens.
As part of that process, characters may be removed, such as punctuation, exclamation
marks, etc.
An example of that could be to split a sentence into words by splitting the string whenever
a whitespace is encountered.

- Tokenizer Filter receive the tokens that the tokenizer produced as input and they may add, remove,
or modify tokens.
As with character filters, an analyzer may contain zero or more token filters, and they
are applied in the order in which they are specified.
The simplest possible example of a token filter is probably the “lowercase” filter, which
lowercases all letters.

#### Using Analyze API
- There is a very convenient API that we can use to test how Elasticsearch analyzes
a given string.

The endpoint is ```“_analyze,”``` and the HTTP verb should be ```POST```.
```JS
POST /_analyze
{
  "text": [
    "Hi", "   Hi this is test", "!!! !!! H!!!",
    "FINE"
    ],
  "analyzer": "standard" //if not then default is used 
}
```

- We can also specify the parts making up an analyzer, being character filters, a tokenizer, and token filters.
- Let’s replicate the “standard” analyzer by specifying its components explicitly.
```
POST /_analyze
{
  "text": [
    "Hi", "   Hi this is test", "!!! !!! H!!!",
    "FINE"
    ],
  "char_filter": [""],
  "tokenizer": "standard",
  "filter": ["lowercase"]
}
```